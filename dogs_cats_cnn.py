# Dogs vs Cats SÄ±nÄ±flandÄ±rma Projesi
# Akbank Derin Ã–ÄŸrenme Bootcamp

# Bu projede kÃ¶pek ve kedi resimlerini ayÄ±rt etmek iÃ§in CNN modeli geliÅŸtireceÄŸiz.
# Veri seti olarak Kaggle'Ä±n Ã¼nlÃ¼ Dogs vs Cats yarÄ±ÅŸmasÄ±nÄ±n veri setini kullanacaÄŸÄ±z.

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import classification_report, confusion_matrix
import os
import warnings
warnings.filterwarnings('ignore')

print("TensorFlow versiyonu:", tf.__version__)
print("GPU mevcut mu:", tf.config.list_physical_devices('GPU'))

# ## 1. Veri Seti Ä°ncelemesi ve HazÄ±rlÄ±k

#Train/Test/Validation zaten ayrÄ±lmÄ±ÅŸ
base_dir = "dataset/"
train_dir = os.path.join(base_dir, 'train')
test_dir = os.path.join(base_dir, 'test')
validation_dir = os.path.join(base_dir, 'validation')

print("Veri seti yapÄ±sÄ±nÄ± kontrol ediyoruz...")
print(f"Train klasÃ¶rÃ¼: {os.path.exists(train_dir)}")
print(f"Test klasÃ¶rÃ¼: {os.path.exists(test_dir)}")
print(f"Validation klasÃ¶rÃ¼: {os.path.exists(validation_dir)}")

if os.path.exists(train_dir):
    print(f"Train'de bulunan sÄ±nÄ±flar: {os.listdir(train_dir)}")
    for class_name in os.listdir(train_dir):
        class_path = os.path.join(train_dir, class_name)
        if os.path.isdir(class_path):
            print(f"  {class_name}: {len(os.listdir(class_path))} resim")

print(f"KullanÄ±lan veri yolu: {train_dir}")

# ## 2. Veri Ã–niÅŸleme ve GÃ¶rselleÅŸtirme

# Bu veri setinde validation zaten var, manual split yapmaya gerek yok
train_datagen = ImageDataGenerator(
    rescale=1./255,           # Piksel deÄŸerlerini 0-1 arasÄ±na normalize et
    rotation_range=25,        # GÃ¶rÃ¼ntÃ¼leri rastgele dÃ¶ndÃ¼r
    width_shift_range=0.1,    # GeniÅŸlik yÃ¶nÃ¼nde kaydÄ±r
    height_shift_range=0.1,   # YÃ¼kseklik yÃ¶nÃ¼nde kaydÄ±r
    horizontal_flip=True,     # Yatay Ã§evir
    zoom_range=0.2           # Rastgele yakÄ±nlaÅŸtÄ±r/uzaklaÅŸtÄ±r
    # validation_split KALDIRDIK Ã§Ã¼nkÃ¼ ayrÄ± validation klasÃ¶rÃ¼ var
)

# Test ve validation verisi iÃ§in sadece normalizasyon
validation_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)

# Veri yÃ¼kleyicileri oluÅŸtur
img_width, img_height = 150, 150
batch_size = 32

train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(img_width, img_height),
    batch_size=batch_size,
    class_mode='binary',  # Ä°ki sÄ±nÄ±f: kÃ¶pek ve kedi
    seed=42  # Tekrarlanabilir sonuÃ§lar iÃ§in
)

# ArtÄ±k ayrÄ± validation generator kullanabiliyoruz!
validation_generator = validation_datagen.flow_from_directory(
    validation_dir,
    target_size=(img_width, img_height),
    batch_size=batch_size,
    class_mode='binary',
    seed=42
)

print("SÄ±nÄ±f etiketleri:", train_generator.class_indices)
print("EÄŸitim Ã¶rnekleri:", train_generator.samples)
print("Validation Ã¶rnekleri:", validation_generator.samples)

# Ã–rnek gÃ¶rÃ¼ntÃ¼leri gÃ¶rselleÅŸtir
def show_sample_images():
    plt.figure(figsize=(12, 8))
    sample_batch = next(train_generator)
    images, labels = sample_batch

    for i in range(8):
        plt.subplot(2, 4, i+1)
        plt.imshow(images[i])
        plt.title(f"{'KÃ¶pek' if labels[i] == 1 else 'Kedi'}")
        plt.axis('off')

    plt.suptitle('Ã–rnek EÄŸitim GÃ¶rÃ¼ntÃ¼leri', fontsize=16)
    plt.tight_layout()
    plt.show()

show_sample_images()

# ## 3. CNN Modelinin OluÅŸturulmasÄ±

# Derin Ã¶ÄŸrenme iÃ§in Convolutional Neural Network mimarisi tasarlÄ±yoruz
# Her katmanÄ±n gÃ¶revi farklÄ± seviyede Ã¶zellik Ã§Ä±karÄ±mÄ± yapmak

model = Sequential([
    # Ä°lk konvolÃ¼syon bloÄŸu - temel kenarlĞ°Ñ€ ve ÅŸekilleri Ã¶ÄŸrenir
    Conv2D(32, (3, 3), activation='relu', input_shape=(img_width, img_height, 3)),
    MaxPooling2D(pool_size=(2, 2)),

    # Ä°kinci blok - daha karmaÅŸÄ±k desenler
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2)),

    # ÃœÃ§Ã¼ncÃ¼ blok - yÃ¼ksek seviye Ã¶zellikler
    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2)),

    # DÃ¶rdÃ¼ncÃ¼ blok - Ã§ok detaylÄ± Ã¶zellikler
    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2)),

    # SÄ±nÄ±flandÄ±rma katmanlarÄ±
    Flatten(),                          # 2D'den 1D'ye dÃ¶nÃ¼ÅŸtÃ¼r
    Dropout(0.5),                       # Overfitting'i Ã¶nle
    Dense(512, activation='relu'),      # Tam baÄŸlantÄ±lÄ± katman
    Dense(1, activation='sigmoid')      # Binary classification iÃ§in sigmoid
])

# Modeli derle
model.compile(
    optimizer=Adam(learning_rate=0.0001),  # Adam optimizer ile Ã¶ÄŸrenme hÄ±zÄ±
    loss='binary_crossentropy',           # Binary sÄ±nÄ±flandÄ±rma iÃ§in loss
    metrics=['accuracy']                  # BaÅŸarÄ±m metriÄŸi
)

# Model mimarisini gÃ¶rÃ¼ntÃ¼le
model.summary()

# Toplam parametre sayÄ±sÄ±nÄ± hesapla
total_params = model.count_params()
print(f"\nToplam parametre sayÄ±sÄ±: {total_params:,}")

# ## 4. Modelin EÄŸitilmesi

# Callback'ler tanÄ±mla - eÄŸitimi kontrol etmek iÃ§in
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

callbacks = [
    EarlyStopping(
        monitor='val_loss',
        patience=5,          # 5 epoch boyunca iyileÅŸme yoksa dur
        restore_best_weights=True
    ),
    ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.2,          # Learning rate'i %20'ye dÃ¼ÅŸÃ¼r
        patience=3,
        min_lr=0.0001
    )
]

print("Model eÄŸitimi baÅŸlÄ±yor...")
print("Bu iÅŸlem birkaÃ§ dakika sÃ¼rebilir...")

# Modeli eÄŸit
epochs = 15
history = model.fit(
    train_generator,
    steps_per_epoch=train_generator.samples // batch_size,
    epochs=epochs,
    validation_data=validation_generator,
    validation_steps=validation_generator.samples // batch_size,
    callbacks=callbacks,
    verbose=1
)

print("EÄŸitim tamamlandÄ±!")

# ## 5. EÄŸitim SonuÃ§larÄ±nÄ±n Analizi

# EÄŸitim sÃ¼recini gÃ¶rselleÅŸtir
def plot_training_history(history):
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))

    # Accuracy grafiÄŸi
    axes[0, 0].plot(history.history['accuracy'], 'bo-', label='EÄŸitim Accuracy')
    axes[0, 0].plot(history.history['val_accuracy'], 'ro-', label='Validation Accuracy')
    axes[0, 0].set_title('Model Accuracy DeÄŸiÅŸimi')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Accuracy')
    axes[0, 0].legend()
    axes[0, 0].grid(True)

    # Loss grafiÄŸi
    axes[0, 1].plot(history.history['loss'], 'bo-', label='EÄŸitim Loss')
    axes[0, 1].plot(history.history['val_loss'], 'ro-', label='Validation Loss')
    axes[0, 1].set_title('Model Loss DeÄŸiÅŸimi')
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('Loss')
    axes[0, 1].legend()
    axes[0, 1].grid(True)

    # Learning Rate (eÄŸer kayÄ±tlÄ± ise)
    if 'lr' in history.history:
        axes[1, 0].plot(history.history['lr'], 'go-')
        axes[1, 0].set_title('Learning Rate DeÄŸiÅŸimi')
        axes[1, 0].set_xlabel('Epoch')
        axes[1, 0].set_ylabel('Learning Rate')
        axes[1, 0].set_yscale('log')
        axes[1, 0].grid(True)

    # Overfitting analizi
    train_acc = np.array(history.history['accuracy'])
    val_acc = np.array(history.history['val_accuracy'])
    overfitting = train_acc - val_acc

    axes[1, 1].plot(overfitting, 'mo-')
    axes[1, 1].set_title('Overfitting Analizi (Train_Acc - Val_Acc)')
    axes[1, 1].set_xlabel('Epoch')
    axes[1, 1].set_ylabel('Accuracy FarkÄ±')
    axes[1, 1].axhline(y=0, color='k', linestyle='--', alpha=0.5)
    axes[1, 1].grid(True)

    plt.tight_layout()
    plt.show()

plot_training_history(history)

# En iyi sonuÃ§larÄ± yazdÄ±r
max_val_acc = max(history.history['val_accuracy'])
min_val_loss = min(history.history['val_loss'])
print(f"\nEn iyi validation accuracy: {max_val_acc:.4f}")
print(f"En dÃ¼ÅŸÃ¼k validation loss: {min_val_loss:.4f}")

# ## 6. Model DeÄŸerlendirmesi

# Validation seti Ã¼zerinde detaylÄ± deÄŸerlendirme yapalÄ±m
print("Validation seti Ã¼zerinde tahmin yapÄ±lÄ±yor...")

# TÃ¼m validation verisi iÃ§in tahmin
validation_generator.reset()
val_predictions = model.predict(validation_generator, verbose=1)
val_predictions_binary = (val_predictions > 0.5).astype(int).flatten()
val_labels = validation_generator.labels

# Confusion Matrix hesapla ve gÃ¶rselleÅŸtir
cm = confusion_matrix(val_labels, val_predictions_binary)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Kedi (0)', 'KÃ¶pek (1)'],
            yticklabels=['Kedi (0)', 'KÃ¶pek (1)'])
plt.title('Confusion Matrix - KarmaÅŸÄ±klÄ±k Matrisi')
plt.ylabel('GerÃ§ek Etiket')
plt.xlabel('Tahmin Edilen Etiket')
plt.show()

# DetaylÄ± classification report
print("\n=== DETAYLI DEÄERLENDÄ°RME RAPORU ===")
print(classification_report(val_labels, val_predictions_binary,
                          target_names=['Kedi', 'KÃ¶pek'], digits=4))

# Accuracy hesapla
accuracy = np.mean(val_labels == val_predictions_binary)
print(f"\nValidation Accuracy: {accuracy:.4f}")

# YanlÄ±ÅŸ sÄ±nÄ±flandÄ±rÄ±lan Ã¶rnekleri gÃ¶ster
def show_misclassified_images():
    plt.figure(figsize=(15, 10))

    # YanlÄ±ÅŸ tahminleri bul
    wrong_indices = np.where(val_labels != val_predictions_binary)[0]

    if len(wrong_indices) > 0:
        # Ä°lk 8 yanlÄ±ÅŸ tahmin
        for i, idx in enumerate(wrong_indices[:8]):
            plt.subplot(2, 4, i+1)

            # GÃ¶rÃ¼ntÃ¼yÃ¼ al
            img_batch, _ = validation_generator[idx // batch_size]
            img_idx = idx % batch_size
            img = img_batch[img_idx]

            plt.imshow(img)
            true_label = 'KÃ¶pek' if val_labels[idx] == 1 else 'Kedi'
            pred_label = 'KÃ¶pek' if val_predictions_binary[idx] == 1 else 'Kedi'
            confidence = val_predictions[idx][0]

            plt.title(f'GerÃ§ek: {true_label}\nTahmin: {pred_label}\nGÃ¼ven: {confidence:.2f}',
                     color='red')
            plt.axis('off')

        plt.suptitle('YanlÄ±ÅŸ SÄ±nÄ±flandÄ±rÄ±lan Ã–rnekler', fontsize=16)
        plt.tight_layout()
        plt.show()
    else:
        print("TÃ¼m tahminler doÄŸru!")

show_misclassified_images()

# ## 7. Grad-CAM ile GÃ¶rselleÅŸtirme

# Modelin hangi bÃ¶lgelere odaklandÄ±ÄŸÄ±nÄ± gÃ¶rselleÅŸtirmek iÃ§in Grad-CAM kullanacaÄŸÄ±z
def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):
    """Grad-CAM heatmap oluÅŸturur"""

    # Gradient modeli oluÅŸtur
    grad_model = tf.keras.models.Model(
        [model.inputs],
        [model.get_layer(last_conv_layer_name).output, model.outputs]  # outputs kullandÄ±k
    )

    # Gradient hesapla
    with tf.GradientTape() as tape:
        last_conv_layer_output, preds = grad_model(img_array, training=False)
        preds = preds[0]  # batch boyutunu indir
        if pred_index is None:
            pred_index = int(tf.argmax(preds))  # tensor -> int dÃ¶nÃ¼ÅŸÃ¼mÃ¼
        class_channel = preds[pred_index]

    # Son konvolÃ¼syon katmanÄ±na gÃ¶re gradient
    grads = tape.gradient(class_channel, last_conv_layer_output)

    # Global average pooling
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))

    # Heatmap hesapla
    last_conv_layer_output = last_conv_layer_output[0]
    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]
    heatmap = tf.squeeze(heatmap)

    # Normalize et
    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)
    return heatmap.numpy()


# Ã–rnek gÃ¶rÃ¼ntÃ¼ler Ã¼zerinde Grad-CAM uygula
def visualize_gradcam_examples():
    plt.figure(figsize=(15, 12))

    # BazÄ± Ã¶rnek gÃ¶rÃ¼ntÃ¼ler al
    validation_generator.reset()
    sample_batch = next(validation_generator)
    images, labels = sample_batch

    for i in range(6):  # 6 Ã¶rnek gÃ¶ster
        img = np.expand_dims(images[i], axis=0)
        pred = model.predict(img, verbose=0)[0][0]

        # Son konvolÃ¼syon katmanÄ±ndan Grad-CAM
        heatmap = make_gradcam_heatmap(img, model, "conv2d_3")  # Son conv katmanÄ±

        # Heatmap'i orijinal boyuta getir
        heatmap_resized = tf.image.resize(heatmap[..., tf.newaxis], (150, 150))
        heatmap_resized = tf.squeeze(heatmap_resized)

        # GÃ¶rselleÅŸtir
        plt.subplot(3, 6, i+1)
        plt.imshow(images[i])
        plt.title(f'Orijinal\n{"KÃ¶pek" if labels[i] == 1 else "Kedi"}')
        plt.axis('off')

        plt.subplot(3, 6, i+7)
        plt.imshow(heatmap_resized, cmap='jet')
        plt.title(f'Heatmap\nGÃ¼ven: {pred:.2f}')
        plt.axis('off')

        plt.subplot(3, 6, i+13)
        plt.imshow(images[i])
        plt.imshow(heatmap_resized, cmap='jet', alpha=0.4)
        plt.title(f'Overlay\n{"KÃ¶pek" if pred > 0.5 else "Kedi"}')
        plt.axis('off')

    plt.suptitle('Grad-CAM Analizi - Model Nerelere BakÄ±yor?', fontsize=16)
    plt.tight_layout()
    plt.show()

print("Grad-CAM gÃ¶rselleÅŸtirmesi hazÄ±rlanÄ±yor...")
visualize_gradcam_examples()

# ## 8. Hiperparametre Optimizasyonu Denemeleri

# FarklÄ± hiperparametreler deneyerek en iyi sonucu bulmaya Ã§alÄ±ÅŸalÄ±m
print("\n=== HÄ°PERPARAMETRE OPTÄ°MÄ°ZASYONU ===")

hyperparams_results = {}

# FarklÄ± learning rate'ler dene
learning_rates = [0.001, 0.0001, 0.00001]

for lr in learning_rates:
    print(f"\nLearning Rate: {lr} test ediliyor...")

    # Basit model oluÅŸtur
    test_model = Sequential([
        Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),
        MaxPooling2D(2, 2),
        Conv2D(64, (3, 3), activation='relu'),
        MaxPooling2D(2, 2),
        Conv2D(128, (3, 3), activation='relu'),
        MaxPooling2D(2, 2),
        Flatten(),
        Dropout(0.5),
        Dense(512, activation='relu'),
        Dense(1, activation='sigmoid')
    ])

    test_model.compile(
        optimizer=Adam(learning_rate=lr),
        loss='binary_crossentropy',
        metrics=['accuracy']
    )

    # KÄ±sa eÄŸitim (test iÃ§in)
    hist = test_model.fit(
        train_generator,
        steps_per_epoch=50,  # HÄ±zlÄ± test iÃ§in
        epochs=5,
        validation_data=validation_generator,
        validation_steps=25,
        verbose=0
    )

    best_val_acc = max(hist.history['val_accuracy'])
    hyperparams_results[f'lr_{lr}'] = best_val_acc
    print(f"En iyi validation accuracy: {best_val_acc:.4f}")

    # BelleÄŸi temizle
    del test_model
    tf.keras.backend.clear_session()

# SonuÃ§larÄ± gÃ¶ster
print("\n=== HÄ°PERPARAMETRE SONUÃ‡LARI ===")
for param, score in hyperparams_results.items():
    print(f"{param}: {score:.4f}")

# ## 9. SonuÃ§ ve Yorumlar

print("\n" + "="*50)
print("PROJE SONUÃ‡ Ã–ZETÄ°")
print("="*50)

final_val_acc = max(history.history['val_accuracy'])
final_train_acc = max(history.history['accuracy'])

print(f"Final Validation Accuracy: {final_val_acc:.4f}")
print(f"Final Training Accuracy: {final_train_acc:.4f}")
print(f"Overfitting Durumu: {'Var' if final_train_acc - final_val_acc > 0.1 else 'Kontrol AltÄ±nda'}")
print(f"Model Parametreleri: {total_params:,}")

print("\nKullanÄ±lan Teknikler:")
print("âœ“ Data Augmentation (Rotation, Flip, Shift, Zoom)")
print("âœ“ Dropout ile Regularization")
print("âœ“ Early Stopping")
print("âœ“ Learning Rate Scheduling")
print("âœ“ Grad-CAM GÃ¶rselleÅŸtirme")
print("âœ“ Hiperparametre Optimizasyonu")

print("\nModel BaÅŸarÄ± Durumu:")
if final_val_acc > 0.90:
    print("ğŸ‰ MÃ¼kemmel! Model Ã§ok iyi Ã§alÄ±ÅŸÄ±yor.")
elif final_val_acc > 0.85:
    print("âœ… Ä°yi! Model baÅŸarÄ±lÄ± sayÄ±lÄ±r.")
elif final_val_acc > 0.80:
    print("âš ï¸  Orta! Daha fazla eÄŸitim gerekebilir.")
else:
    print("âŒ DÃ¼ÅŸÃ¼k! Model mimarisi veya hiperparametreler gÃ¶zden geÃ§irilmeli.")

print("\nProje tamamlandÄ±! ğŸš€")
